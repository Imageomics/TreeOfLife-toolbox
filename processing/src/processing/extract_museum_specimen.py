"""
extract_museum_specimen.py

Similar to `extract_images.py`, this script is specialized to extract GBIF museum specimen records that have multiple images per occurrence from the TOL-200M dataset using a provided lookup table. 
The lookup table is expected to be generated by the `create_lookup_tbl.py` script. The script processes each group specified in the lookup table, filters and joins relevant data, and writes the result to Parquet files.
Additionally, it can repartition the extracted result by specimen category.
- Plant
- Fungi
- Insect
- Invertebrate Zoology
- Vertebrate Zoology - Fishes
- Vertebrate Zoology - Amphibians
- Vertebrate Zoology - Reptiles
- Vertebrate Zoology - Birds
- Vertebrate Zoology - Mammals
- Vertebrate Zoology - Others
- Microbiology
- Uncategorized

Usage:
    python extract_museum_specimen.py [--repartition]

Arguments:
    --repartition: Optional flag to repartition the extracted museum specimen images by specimen categories. ONLY USE AFTER EXTRACTING IMAGES.

Dependencies:
    - os
    - logging
    - argparse
    - pyspark

Functions:
    init_spark():
        Initializes and returns a SparkSession.

    process_group(spark, base_input_path, base_output_path, group_id):
        Processes a specific group by filtering and joining relevant data, and writes the result to Parquet.

    repartition_by_specimen_category(spark, result_df, output_path, partition_size=1500):
        Repartitions the extracted museum specimen by specimen category and writes the result to disk.

    main(repartition=False):
        Main function to extract GBIF museum specimen records that have multiple images per occurrence from the TOL-200M dataset using the provided lookup table.

Example:
    python extract_museum_specimen.py --repartition
"""

import logging
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import broadcast, col, when, ceil, desc
import os
import time
import argparse

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

N_EXECUTORS = 80

# def init_spark() -> SparkSession:
#     spark = (SparkSession.builder
#              .appName("GBIF EDA")
#              .config("spark.executor.instances", "80")
#              .config("spark.executor.memory", "75G")
#              .config("spark.executor.cores", "12")
#              .config("spark.sql.parquet.enableVectorizedReader", "false") 
#              .getOrCreate())
    
#     return spark

def init_spark() -> SparkSession:

    spark = (
        SparkSession.builder
        .appName("GBIF EDA")
        .config("spark.executor.instances", "80")
        .config("spark.executor.memory", "75G")
        .config("spark.executor.cores", "12")
        .config("spark.driver.memory", "64G")
        # Additional Tunning
        .config("spark.sql.shuffle.partitions", "1000")
        #.config("spark.local.dir", "/fs/scratch/PAS2136/netzissou/spark_temp")
        #.config("spark.sql.files.maxPartitionBytes", "256MB")
        .config("spark.sql.parquet.enableVectorizedReader", "false") 
        .config("spark.sql.parquet.compression.codec", "snappy")
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        .getOrCreate()
    )
    
    return spark

def process_group(spark, base_input_path, base_output_path, group_id):

    # Construct paths for the current group
    group_input_path = f"{base_input_path}/group_id={group_id}"
    group_output_path = f"{base_output_path}/group_id={group_id}"


    filtered_df = spark.read.parquet(group_input_path)
    unique_paths = [row['path'] for row in filtered_df.select("path").distinct().collect()]
    
    # Read the combined DataFrame from unique paths
    combined_df = spark.read.parquet(*unique_paths).select(["uuid", "original_size", "resized_size", "image"])

    result_df = combined_df.join(broadcast(filtered_df), on="uuid", how="inner")
    result_df = result_df.dropDuplicates(["uuid"]).repartition(100)
    
    # Write the result to the output path
    result_df.write.mode("overwrite").parquet(group_output_path)
    print(f"Processed and saved results for group_id={group_id} to {group_output_path}")

def repartition_by_specimen_category(spark: SparkSession, result_df: DataFrame, output_path: str, partition_size: int = 1500):
    """
    Repartition the extracted museum specimen by specimen category.

    Args:
        spark (SparkSession): active SparkSession
        result_df (DataFrame): DataFrame containing the extracted museum specimen
        output_path (str): Output path to save the repartitioned data
        partition_size (int, optional): Size of each partition. Defaults to 1500.
    """
    fish_classes = [
        "Agnatha",                    # Jawless fish
        "Myxini",                     # Hagfish
        "Pteraspidomorphi",           # Early jawless fish (extinct)
        "Thelodonti",                 # Extinct
        "Anaspida",                   # Extinct
        "Petromyzontida",             # Lampreys
        "Hyperoartia",
        "Conodonta",                  # Conodonts (extinct)
        "Cephalaspidomorphi",         # Early jawless fish (extinct)
        "Placodermi",                 # Armoured fish (extinct)
        "Acanthodii",                 # Spiny sharks (extinct)
        "Actinopterygii",             # Ray-finned fish
        "Sarcopterygii"               # Lobe-finned fish
        "Chondrichthyes",             # cartilaginous fish
        "Sarcopterygii"              # lobe-finned fish
    ]

    fish_orders = [
        # Class Myxini
        "Myxiniformes",
        # Class Cephalaspidomorphi
        "Petromyzontiformes",
        
        # Class Chondrichthyes (Cartilaginous Fishes)
        "Selachii",          # Sharks
        "Batoidei",          # Rays, sawfishes, guitarfishes, skates, stingrays
        "Chimaeriformes",    # Chimaeras

        # Class Actinopterygii (Ray-Finned Fishes)
        # Subclass Chondrostei
        "Acipenseriformes",  # Sturgeons and paddlefishes
        "Polypteriformes",   # Bichirs and reedfish

        # Infraclass Holostei
        "Amiiformes",        # Bowfins
        "Semionotiformes",   # Gars

        # Infraclass Teleostei (Advanced Bony Fishes)
        # Superorder Osteoglossomorpha
        "Osteoglossiformes", # Bonytongues, mooneyes, knife fishes, mormyrs

        # Superorder Elopomorpha
        "Elopiformes",       # Ladyfishes and tarpons
        "Albuliformes",      # Bonefishes
        "Anguilliformes",    # eels
        "Saccopharyngiformes",# gulpers

        # Superorder Clupeomorpha
        "Clupeiformes",      # Herrings and anchovies

        # Superorder Ostariophysi
        "Gonorynchiformes",  # Milkfishes
        "Cypriniformes",     # Carps, minnows, loaches
        "Characiformes",     # Characins, tetras, piranhas
        "Siluriformes",      # Catfishes
        "Gymnotiformes",     # Knifefishes, electric eels

        # Superorder Protacanthopterygii
        "Salmoniformes",     # Salmons, trouts, and allies
        "Esociformes",       # Pikes and pickerels
        "Osmeriformes",      # Argentines and smelts

        # Superorder Paracanthopterygii
        "Percopsiformes",    # Trout-perches, pirate perches, cave fishes
        "Gadiformes",        # Cods and allies
        "Lophiiformes",      # Anglerfishes

        "Stomiiformes",
        "Ateleopodiformes",
        "Aulopiformes",
        "Myctophiformes",
        "Lampriformes",
        "Polymixiiformes",
        "Percopsiformes",
        "Gadiformes",
        "Batrachoidiformes",
        "Lophiiformes",
        "Ophidiiformes",
        "Atheriniformes",
        "Cyprinodontiformes",
        "Beloniformes",
        "Mugiliformes",
        "Stephanoberyciformes",
        "Beryciformes",
        "Zeiformes",
        "Gasterosteiformes",
        "Synbranchiformes",
        "Scorpaeniformes",
        "Perciformes",
        "Pleuronectiformes",
        "Tetraodontiformes",
        "Coelacanthiformes",
        "Ceratodontiformes",
        "Lepidosireniformes"
    ]

    invertebrate_phyla = [
        "Mollusca", "Arthropoda", "Bryozoa", "Cnidaria", "Brachiopoda", "Echinodermata",
        "Porifera", "Foraminifera", "Tardigrada", "Annelida", "Nemertea", "Nematoda",
        "Platyhelminthes", "Sipuncula", "Rotifera", "Chaetognatha", "Gastrotricha",
        "Hemichordata", "Onychophora", "Xenacoelomorpha", "Ctenophora", "Priapulida",
        "Acanthocephala", "Phoronida", "Entoprocta", "Nematomorpha", "Loricifera", "Kinorhyncha", "Cycliophora"
    ]

    microbiology_phyla = list(set([
        "Proteobacteria", "Cyanobacteria", "Actinobacteriota", "Firmicutes", "Firmicutes_C",
        "Myzozoa", "Cercozoa", "Bigyra", "Ciliophora", "Haptophyta", "Metamonada",
        "Euglenozoa", "Sarcomastigophora", "Bacteroidota", "Amoebozoa", "Microsporidia",
        "Cryptophyta", "Myxococcota", "Fusobacteriota"
    ]))

    cond_plant = (
        ((col("kingdom").isNotNull()) & (col("kingdom") == "Plantae")) |
        ((col("phylum").isNotNull()) & (col("phylum").isin(["Magnoliopsida", "Ochrophyta"]))) # sea weed
    )
    cond_fungi = (
        ((col("kingdom").isNotNull()) & (col("kingdom") == "Fungi")) |
        ((col("phylum").isNotNull()) & (col("phylum").isin(["Mycetozoa", "Oomycota", "Leotiomycetes", "Sordariomycetes"])))
    )
    cond_insect = (col("class").isNotNull()) & (col("class") == "Insecta")
    cond_invertebrate = (
        (col("phylum").isNotNull()) & 
        (col("phylum").isin(invertebrate_phyla)) & 
        ((col("class").isNull()) | (col("class") != "Insecta"))
    )

    cond_amphibian = (
        (col("phylum").isNotNull()) & 
        (col("class").isNotNull()) &
        (col("phylum") == "Chordata") &
        (col("class") == "Amphibia")
    )
    cond_reptile = (
        (col("phylum").isNotNull()) & 
        (col("class").isNotNull()) &
        (col("phylum") == "Chordata") &
        (col("class") == "Reptilia")
    )
    cond_bird = (
        (col("phylum").isNotNull()) & 
        (col("class").isNotNull()) &
        (col("phylum") == "Chordata") &
        (col("class") == "Aves")
    )
    cond_fish = (
        (col("phylum").isNotNull()) & 
        (col("order").isNotNull()) &
        (col("phylum") == "Chordata") &
        (col("order").isin(fish_orders))
    )
    cond_mammal = (
        (col("phylum").isNotNull()) & 
        (col("class").isNotNull()) &
        (col("phylum") == "Chordata") &
        (col("class") == "Mammalia")
    )

    # cond_paleobiology = (
    #     (col("phylum").isNotNull()) &
    #     (col("phylum").isin(
    #         "Brachiopoda", "Foraminifera"
    #     ))
    # )

    cond_microbiology = (
        (col("phylum").isNotNull()) &
        (col("phylum").isin(microbiology_phyla))
    )

    result_df_categorized = result_df.withColumn(
            "category",
            when(cond_plant, "Plant")
            .when(cond_fungi, "Fungi")  # Assuming fungi are classified under paleobiology
            .when(cond_insect, "Insect")
            .when(cond_invertebrate, "Invertebrate Zoology")
            .when(cond_fish, "Vertebrate Zoology - Fishes")
            .when(cond_amphibian, "Vertebrate Zoology - Amphibians")
            .when(cond_reptile, "Vertebrate Zoology - Reptiles")
            .when(cond_bird, "Vertebrate Zoology - Birds")
            .when(cond_mammal, "Vertebrate Zoology - Mammals")
            .when(((col("phylum").isNotNull()) & (col("phylum") == "Chordata")), "Vertebrate Zoology - Others") 
            #.when(cond_paleobiology, "Paleobiology")
            .when(cond_microbiology, "Microbiology")
            .otherwise("Uncategorized")
        )
    
    # result_df_categorized = result_df_categorized.repartition("category")

    # (
    #     result_df_categorized
    #     .write
    #     .partitionBy("category")
    #     .mode("overwrite")
    #     .option("maxRecordsPerFile", partition_size)
    #     .parquet("output_path")
    # )

    all_categories = [
        "Plant",
        "Fungi",
        "Insect",
        "Invertebrate Zoology",
        "Vertebrate Zoology - Fishes",
        "Vertebrate Zoology - Amphibians",
        "Vertebrate Zoology - Reptiles",
        "Vertebrate Zoology - Birds",
        "Vertebrate Zoology - Mammals",
        "Vertebrate Zoology - Others",
        "Microbiology",
        "Uncategorized"
    ]

    category_counts = result_df_categorized.groupBy("category").count().orderBy("count").collect()

    for category_row in category_counts:
        category = category_row["category"]
        count = category_row["count"]
        
        start_time = time.time()
        
        # Calculate optimal partition count (at least 1)
        partition_count = max(1, count // partition_size)
        
        print(f"Writing {category} data ({count} records) with {partition_count} partitions...")
        
        (result_df_categorized
            .filter(col("category") == category)
            .repartition(partition_count)  # Try coalescing if OOM
            .write
            .mode("overwrite")
            .option("maxRecordsPerFile", partition_size)
            .parquet(f"{output_path}/category={category}"))
        
        elapsed_time = time.time() - start_time
        print(f"Finished writing {category} in {elapsed_time:.2f} seconds.\n")


def main(target_dir: str, output_dir: str, repartition=False):
    
    
    spark = init_spark()

    # Repartition the result by specimen category
    if repartition:
        print("Repartitioning the extracted museum specimen images by specimen categories...")
        PARTITION_SIZE = 1500
        result_df = spark.read.parquet(target_dir)
        
        repartition_by_specimen_category(
            spark,
            result_df,
            output_dir,
            partition_size=PARTITION_SIZE
        )
        print(f"Repartitioned data saved to {output_dir}")
        return # Exit
    
    # Extract museum specimen images from TOL-200M
    base_input_path = target_dir                   # Lookup table path
    base_output_path = output_dir

    group_id_list = sorted(
        [int(f.split("=")[-1]) for f in os.listdir(base_input_path) if f.startswith("group_id=")]
    )

    # Filter the finised the batches
    START_BATCH_INDEX = 55
    group_id_list = [x for x in group_id_list if x>START_BATCH_INDEX]

    # Process each group
    for group_id in group_id_list:
        process_group(spark, base_input_path, base_output_path, str(group_id))
    
    


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Extract images from TOL-200M using the provided lookup table.")
    parser.add_argument( "target_dir", type=str, help="Target directory for the lookup table or the extracted data if it's for repartitioning.")
    parser.add_argument("output_dir", type=str, help="Output directory for the extracted data.")
    parser.add_argument("--repartition", action="store_true", help="Repartition the extracted museum specimen images by specimen categories.")
    args = parser.parse_args()

    main(
        target_dir=args.target_dir,
        output_dir=args.output_dir,
        repartition=args.repartition
    )
