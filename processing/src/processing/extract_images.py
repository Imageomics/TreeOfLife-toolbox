"""
extract_images.py

This script extracts images subset from the TOL-200M dataset using a provided lookup table. 
The lookup table is expected to be generated by the `create_lookup_tbl.py` script. 
The script processes each lookup table batch:
- Read all unique data file paths from the lookup table as a combined Spark DataFrame.
- Join the combined Spark DataFrame with lookup table to filter relevant records.
- Write the result that contains the images, image metadata, and record metadata to Parquet files.

Usage:
    python extract_images.py <lookup_tbl_path> <output_path> [--resume_batch_index <resume_batch_index>]

Arguments:
    lookup_tbl_path (str): The path to the lookup table.
    output_path (str): The output path to save the extracted images.
    --resume_batch_index (int, optional): The lookup table batch index to resume from.

Dependencies:
    - os
    - gc
    - logging
    - argparse
    - pyspark

Functions:
    init_spark():
        Initializes and returns a SparkSession.

    process_group(spark, base_input_path, base_output_path, group_id):
        Processes a specific group by filtering and joining relevant data, and writes the result to Parquet.

    main(lookup_tbl_path, output_path, resume_batch_index=None):
        Main function to extract images from the TOL-200M dataset using the provided lookup table.

Example:
    python extract_images.py /path/to/lookup_tbl /path/to/output --resume_batch_index 5
"""

import os
import gc
import logging
import argparse
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import broadcast, col, countDistinct, row_number, floor

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)


N_EXECUTORS = 80

COLS_TAXONOMIC = [
    "kingdom", "phylum", "class", "order", "family", "genus", "species"
]

def init_spark() -> SparkSession:

    spark = (
        SparkSession.builder
        .appName("GBIF EDA")
        .config("spark.executor.instances", f"{N_EXECUTORS}")
        .config("spark.executor.memory", "75G")
        .config("spark.executor.cores", "12")
        .config("spark.driver.memory", "64G")
        # Additional Tunning
        .config("spark.sql.shuffle.partitions", "1000")
        #.config("spark.sql.files.maxPartitionBytes", "256MB")
        .config("spark.sql.parquet.enableVectorizedReader", "false") 
        .config("spark.sql.parquet.compression.codec", "snappy")
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        .getOrCreate()
    )
    
    return spark

def process_group(spark, base_input_path, base_output_path, group_id):

    # Optional: Log driver memory
    # log_driver_memory(spark, stage=f"Start of group_id={group_id}")

    # Construct paths for the current group
    group_input_path = f"{base_input_path}/group_id={group_id}"
    group_output_path = f"{base_output_path}/group_id={group_id}"


    try:
        filtered_df = spark.read.parquet(group_input_path)
        #filtered_df = filtered_df.persist()
        unique_paths = [row['path'] for row in filtered_df.select("path").distinct().collect()]

        # Read the combined DataFrame from unique paths
        combined_df = spark.read.parquet(*unique_paths).select(["uuid", "original_size", "resized_size", "image"])

        result_df = combined_df.join(broadcast(filtered_df), on="uuid", how="inner")
        result_df = result_df.dropDuplicates(["uuid"])

        # Repartition
        n_row = result_df.count()
        rows_per_partition = 1024 # 1GB / 1MB per row
        n_partition = max(1, n_row // rows_per_partition)
        result_df = result_df.repartition(n_partition)

        # Write the result to the output path
        result_df.write.mode("overwrite").parquet(group_output_path)
        # Use print if no logger
        logging.info(f"Processed and saved results for group_id={group_id} to {group_output_path}")

    except Exception as e:
        logging.error(f"Error processing group_id={group_id}: {e}")
        raise

    finally:
        # Free memory
        filtered_df.unpersist()
        result_df.unpersist()

        unique_paths = None
        filtered_df = None
        result_df = None
        combined_df = None

        # spark.catalog.clearCache()
        # gc.collect()


def main(lookup_tbl_path: str, output_path: str, resume_batch_index: int = None):
    
    spark = init_spark()
    
    batch_id_list = sorted(
        [int(f.split("=")[-1]) for f in os.listdir(lookup_tbl_path) if f.startswith("group_id=")]
    )

    # Filter the finised the batches
    if resume_batch_index is not None:
        batch_id_list = [x for x in batch_id_list if x>resume_batch_index] 
    
    # Process each batch
    for id in batch_id_list:
        process_group(spark, lookup_tbl_path, output_path, str(id))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Extract images from TOL-200M using the provided lookup table.")
    parser.add_argument("lookup_tbl_path", type=str, help="The path to the lookup table.")
    parser.add_argument("output_path", type=str, help="The output path.")
    parser.add_argument("--resume_batch_index", type=int, help="The lookup table batch index to resume from.")
    args = parser.parse_args()

    main(args.lookup_tbl_path, args.output_path, args.resume_batch_index)