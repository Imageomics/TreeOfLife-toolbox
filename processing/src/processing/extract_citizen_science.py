"""
extract_citizen_science.py

Similar to `extract_images.py`, this script is specialized to extract GBIF citizen science images from the TOL-200M dataset using a provided lookup table, due to high amount of citizen science images.
The lookup table is expected to be generated by the `create_lookup_tbl.py` script. The script processes each group specified in the lookup table, filters and joins relevant data, and writes the result to Parquet files. 

Usage:
    python extract_citizen_science.py

Dependencies:
    - os
    - gc
    - logging
    - pyspark

Functions:
    init_spark():
        Initializes and returns a SparkSession.

    log_driver_memory(spark, stage=""):
        Logs the available driver memory using JVM utilities in GB.

    process_group(spark, base_input_path, base_output_path, group_id):
        Processes a specific group by filtering and joining relevant data, and writes the result to Parquet.

    main():
        Main function to extract citizen science images from the TOL-200M dataset using the provided lookup table.

Example:
    python extract_citizen_science.py
"""

import os
import gc
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import broadcast


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)


N_EXECUTORS = 80

def init_spark() -> SparkSession:

    spark = (
        SparkSession.builder
        .appName("GBIF EDA")
        .config("spark.executor.instances", "80")
        .config("spark.executor.memory", "75G")
        .config("spark.executor.cores", "12")
        .config("spark.driver.memory", "64G")
        # Additional Tunning
        .config("spark.sql.shuffle.partitions", "1000")
        #.config("spark.sql.files.maxPartitionBytes", "256MB")
        .config("spark.sql.parquet.enableVectorizedReader", "false") 
        .config("spark.sql.parquet.compression.codec", "snappy")
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        .getOrCreate()
    )
    
    return spark

def log_driver_memory(spark, stage=""):
    """
    Logs the available driver memory using JVM utilities in GB.

    Args:
        spark (SparkSession): The active Spark session.
        stage (str): A string to identify the stage where the memory is logged.
    """
    try:
        runtime = spark._jvm.java.lang.Runtime.getRuntime()
        total_memory = runtime.totalMemory() / (1024 ** 3)  # Convert to GB
        free_memory = runtime.freeMemory() / (1024 ** 3)    # Convert to GB
        max_memory = runtime.maxMemory() / (1024 ** 3)      # Convert to GB

        logging.info(
            f"[{stage}] Driver Memory - Total: {total_memory:.2f} GB, "
            f"Free: {free_memory:.2f} GB, Max: {max_memory:.2f} GB"
        )
    except Exception as e:
        logging.error(f"Failed to log driver memory: {e}")

def process_group(spark, base_input_path, base_output_path, group_id):

    # Optional: Log driver memory
    # log_driver_memory(spark, stage=f"Start of group_id={group_id}")

    # Construct paths for the current group
    group_input_path = f"{base_input_path}/group_id={group_id}"
    group_output_path = f"{base_output_path}/group_id={group_id}"


    try:
        filtered_df = spark.read.parquet(group_input_path)
        #filtered_df = filtered_df.persist()
        unique_paths = [row['path'] for row in filtered_df.select("path").distinct().collect()]

        # Read the combined DataFrame from unique paths
        combined_df = spark.read.parquet(*unique_paths).select(["uuid", "original_size", "resized_size", "image"])

        result_df = combined_df.join(broadcast(filtered_df), on="uuid", how="inner")
        result_df = result_df.dropDuplicates(["uuid"])

        # Repartition
        n_row = result_df.count()
        rows_per_partition = 1024 # 1GB / 1MB per row
        n_partition = n_row // rows_per_partition
        result_df = result_df.repartition(n_partition)

        # Write the result to the output path
        result_df.write.mode("overwrite").parquet(group_output_path)
        # Use print if no logger
        logging.info(f"Processed and saved results for group_id={group_id} to {group_output_path}")

    except Exception as e:
        logging.error(f"Error processing group_id={group_id}: {e}")
        log_driver_memory(spark, stage=f"group_id={group_id}")
        raise

    finally:
        # Free memory
        filtered_df.unpersist()
        result_df.unpersist()

        unique_paths = None
        filtered_df = None
        result_df = None
        combined_df = None

        # spark.catalog.clearCache()
        # gc.collect()


def main():
    
    
    spark = init_spark()
    
    base_input_path = "/fs/scratch/PAS2136/gbif/lookup_tables/2024-05-01/lookup_multi_images_citizen_science"
    base_output_path = "/fs/scratch/PAS2136/gbif/image_lookup/multi_images_citizen_science"

    group_id_list = sorted(
        [int(f.split("=")[-1]) for f in os.listdir(base_input_path) if f.startswith("group_id=")]
    )

    # Filter the finised the batches
    START_BATCH_INDEX = 100
    group_id_list = [x for x in group_id_list if x>START_BATCH_INDEX] 

    # Process each group
    for group_id in group_id_list:
        process_group(spark, base_input_path, base_output_path, str(group_id))
        log_driver_memory(spark, stage=f"Memory status after writing: group_id={group_id}")


if __name__ == "__main__":
    main()
